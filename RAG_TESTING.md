# RAG Implementation Testing Guide

## Test Checklist

Use this guide to verify all RAG functionality works correctly.

---

## 1. Setup & Initialization Tests

### Test 1.1: Vector Store Creation
**Objective:** Verify vector store is created on first RAG access

**Steps:**
1. Go to View Summaries page
2. Click RAG-Based Chat tab
3. Wait for initialization (should see "Initializing RAG Chatbot and vector store...")
4. Watch for success message: "‚úÖ RAG Chatbot initialized successfully!"

**Expected Result:**
- ‚úÖ Vector store is created in `/output_data/vector_store/`
- ‚úÖ Success message appears
- ‚úÖ Vector store files exist on disk
- ‚úÖ Subsequent tab switches don't reinitialize

**Failure Indicators:**
- ‚ùå Error message appears
- ‚ùå No vector store files created
- ‚ùå Timeout during initialization

---

### Test 1.2: API Key Validation
**Objective:** Verify API key requirement is enforced

**Steps:**
1. Clear API key in main app (set to empty or invalid)
2. Go to View Summaries
3. Click RAG-Based Chat tab
4. Observe the message

**Expected Result:**
- ‚úÖ Warning message: "Please enter your OpenAI API key in the main app page first!"
- ‚úÖ No attempt to initialize without valid key
- ‚úÖ Chat input is disabled

**Failure Indicators:**
- ‚ùå Initialization attempts without key
- ‚ùå No warning message
- ‚ùå Confusing error messages

---

## 2. Vector Store Tests

### Test 2.1: Document Preparation
**Objective:** Verify summaries are properly converted to documents

**Steps:**
1. Check logs for vector store creation
2. Look for: "Prepared X documents for vector store"
3. Verify all 5 summaries appear in logs

**Expected Output in Logs:**
```
Loaded 5 summaries from output_data/bulk_summaries.json
Prepared 5 documents for vector store
Creating FAISS vector store with 5 documents...
Vector store saved to output_data/vector_store
Vector store loaded successfully
```

**Expected Result:**
- ‚úÖ All 5 summaries loaded
- ‚úÖ All 5 documents prepared
- ‚úÖ No data loss during conversion
- ‚úÖ Metadata preserved

---

### Test 2.2: Persistence
**Objective:** Verify vector store is saved and reused

**Steps:**
1. First RAG chat access - vector store created
2. Switch to Standard Chat tab
3. Switch back to RAG Chat tab
4. Check logs for load vs create message

**Expected Result:**
- ‚úÖ First access: "Creating FAISS vector store..."
- ‚úÖ Second access: "Loading existing vector store from..."
- ‚úÖ No re-creation on tab switch
- ‚úÖ Instant load on second access

**Performance:**
- ‚è±Ô∏è First creation: 5-10 seconds
- ‚è±Ô∏è Subsequent loads: <1 second

---

### Test 2.3: Vector Store Reload
**Objective:** Verify reload functionality picks up new summaries

**Steps:**
1. Click "üîÑ Reload Vector Store" button
2. Watch for spinner
3. Wait for completion

**Expected Result:**
- ‚úÖ Spinner appears during reload
- ‚úÖ Vector store is force-recreated
- ‚úÖ Success message appears
- ‚úÖ Subsequent queries use updated index

**Logs Should Show:**
```
Reloading vector store...
Creating FAISS vector store with N documents...
```

---

## 3. Similarity Search Tests

### Test 3.1: Basic Query
**Objective:** Verify vector similarity search works

**Steps:**
1. Type question: "Which agents have the highest scores?"
2. Send query
3. Check RAG response

**Expected Result:**
- ‚úÖ Response received within 2-4 seconds
- ‚úÖ Response mentions specific agent names
- ‚úÖ Includes actual agent scores from summaries
- ‚úÖ References specific call IDs

**Example Response Should Include:**
```
Sarah Mitchell (AG-2847): 95/100
Marcus Johnson (AG-2951): 85/100
...
```

---

### Test 3.2: Semantic Search Accuracy
**Objective:** Verify semantic search understands meaning

**Steps:**
1. Ask: "What customer emotions appear in calls?"
2. Ask: "How do customers feel?"
3. Both should retrieve similar, relevant summaries

**Expected Result:**
- ‚úÖ Both questions return similar documents
- ‚úÖ Emotions and sentiments mentioned in response
- ‚úÖ Specific emotions cited from summaries
- ‚úÖ Response is contextually correct

---

### Test 3.3: Complex Question Handling
**Objective:** Verify RAG handles multi-faceted questions

**Steps:**
1. Ask: "Analyze customer sentiment patterns"
2. Observe response structure

**Expected Result:**
- ‚úÖ Response organizes sentiment by emotional categories
- ‚úÖ References multiple calls
- ‚úÖ Provides frequency/pattern analysis
- ‚úÖ Highlights trends

---

## 4. Chat History Tests

### Test 4.1: Message Display
**Objective:** Verify chat messages display correctly

**Steps:**
1. Ask a question
2. Observe message in chat container
3. Ask follow-up question
4. Observe both messages visible

**Expected Result:**
- ‚úÖ User message appears in blue/left
- ‚úÖ Assistant message appears in different style
- ‚úÖ Both messages stay in history
- ‚úÖ Chat container scrolls if needed
- ‚úÖ Auto-scroll shows latest messages

---

### Test 4.2: History Persistence Across Tab Switches
**Objective:** Verify chat history persists when switching tabs

**Steps:**
1. In RAG Chat: Ask a question, get response
2. Switch to Standard Chat tab
3. Switch back to RAG Chat tab
4. Observe history

**Expected Result:**
- ‚úÖ RAG chat history still visible
- ‚úÖ Standard chat history independent
- ‚úÖ No history loss
- ‚úÖ Correct history shown in each tab

---

### Test 4.3: Clear History
**Objective:** Verify clear history button works

**Steps:**
1. Have some chat messages
2. Click "Clear RAG Chat History"
3. Observe chat container

**Expected Result:**
- ‚úÖ Success message appears
- ‚úÖ Chat history completely empty
- ‚úÖ Can start fresh conversation

---

## 5. Context & LLM Tests

### Test 5.1: Citation Accuracy
**Objective:** Verify LLM correctly cites call data

**Steps:**
1. Ask: "Which agents got perfect 5-star ratings?"
2. Check response for call ID and agent name

**Expected Result:**
- ‚úÖ Response mentions Sarah Mitchell (AG-2847)
- ‚úÖ References call CC-2025-001847
- ‚úÖ Cites actual 5-star rating
- ‚úÖ Information matches bulk_summaries.json

---

### Test 5.2: Multi-turn Conversation
**Objective:** Verify context is maintained across questions

**Steps:**
1. Q1: "Which agents have high scores?"
2. Q2: "Tell me about the top performer"
3. Q3: "How did they handle difficult customers?"

**Expected Result:**
- ‚úÖ Q2 references agent from Q1
- ‚úÖ Q3 builds on previous context
- ‚úÖ LLM remembers earlier questions
- ‚úÖ Responses are contextually connected

---

### Test 5.3: System Prompt Integration
**Objective:** Verify system prompt is loaded and used

**Steps:**
1. Check response style and instructions
2. Verify response follows prompt guidelines

**Expected Behavior:**
- ‚úÖ Uses specific agent names in citations
- ‚úÖ Provides quantitative analysis when asked
- ‚úÖ Highlights patterns and trends
- ‚úÖ Organized and clear format
- ‚úÖ References actual metrics

---

## 6. Predefined Questions Tests

### Test 6.1: Button Functionality
**Objective:** Verify predefined question buttons work

**Steps:**
1. Click "Which agents have the highest scores?"
2. Click "What are common customer issues?"
3. Click "Summarize unresolved issues"
4. Try each of 6 predefined buttons

**Expected Result:**
- ‚úÖ Each button triggers a query
- ‚úÖ Questions are accurate/relevant
- ‚úÖ Responses appear in chat
- ‚úÖ Buttons are easy to read
- ‚úÖ Icons display correctly

---

### Test 6.2: Questions Relevance
**Objective:** Verify predefined questions produce good results

**Click Each Button:**
- "Which agents have the highest scores?" ‚Üí Lists agents by score ‚úÖ
- "What are common customer issues?" ‚Üí Lists issue categories ‚úÖ
- "Summarize unresolved issues" ‚Üí Lists unresolved calls ‚úÖ
- "What agent got the best ratings?" ‚Üí Lists by rating ‚úÖ
- "Analyze customer sentiment patterns" ‚Üí Breaks down emotions ‚úÖ
- "Which department needs improvement?" ‚Üí Department analysis ‚úÖ

---

## 7. UI/UX Tests

### Test 7.1: Tab Switching
**Objective:** Verify tabs switch cleanly without errors

**Steps:**
1. Click on "Standard Chat" tab
2. Click on "RAG-Based Chat" tab
3. Repeat several times
4. Verify no console errors

**Expected Result:**
- ‚úÖ Tabs switch instantly
- ‚úÖ No errors in browser console
- ‚úÖ Chat content changes appropriately
- ‚úÖ History persists per tab
- ‚úÖ Session state maintained

---

### Test 7.2: Chat Container Scrolling
**Objective:** Verify auto-scroll works in 400px container

**Steps:**
1. Ask multiple questions (5+ messages)
2. Each response should be visible
3. Latest message should be visible after new response

**Expected Result:**
- ‚úÖ Container stays 400px height
- ‚úÖ Scrollbar appears with many messages
- ‚úÖ Auto-scroll shows latest message
- ‚úÖ Can manually scroll to see older messages
- ‚úÖ No layout breaking

---

### Test 7.3: Button Layout
**Objective:** Verify buttons layout properly on different screen sizes

**Steps:**
1. View on desktop (wide screen)
2. View on tablet
3. View on mobile (narrow screen)
4. Resize browser window

**Expected Result:**
- ‚úÖ Buttons stack appropriately
- ‚úÖ Text readable in all sizes
- ‚úÖ No button overlap
- ‚úÖ "Reload Vector Store" button accessible
- ‚úÖ "Clear Chat History" button accessible

---

## 8. Error Handling Tests

### Test 8.1: Network Error Handling
**Objective:** Verify graceful error handling

**Steps:**
1. Simulate network error (disconnect internet)
2. Try to ask a question in RAG chat
3. Observe error handling

**Expected Result:**
- ‚úÖ User-friendly error message
- ‚úÖ No app crash
- ‚úÖ Can retry when connection restored
- ‚úÖ Error logged appropriately

---

### Test 8.2: Invalid Question Handling
**Objective:** Verify system handles edge cases

**Steps:**
1. Ask: "" (empty question)
2. Ask: "????????" (special characters)
3. Ask: Very long question (500+ characters)
4. Ask: Non-English text (if supported)

**Expected Result:**
- ‚úÖ Empty questions don't send
- ‚úÖ Special characters handled gracefully
- ‚úÖ Long questions processed correctly
- ‚úÖ Clear behavior in all cases

---

### Test 8.3: Model Fallback
**Objective:** Verify default system prompt works if file missing

**Steps:**
1. Temporarily rename `prompt_store/chat_system_prompt.txt`
2. Access RAG chat
3. Ask a question
4. Verify response uses default prompt

**Expected Result:**
- ‚úÖ Default prompt loads automatically
- ‚úÖ Response quality remains good
- ‚úÖ No errors in logs
- ‚úÖ Rename file back after test

---

## 9. Performance Tests

### Test 9.1: Response Time Measurement
**Objective:** Verify acceptable response times

**Steps:**
1. Time first RAG query: 5-10 seconds (includes LLM)
2. Time second RAG query: 2-3 seconds
3. Time predefined button: 2-3 seconds
4. Time vector store reload: 5-10 seconds

**Expected Times:**
- ‚è±Ô∏è Vector store creation: 5-10 seconds (one-time)
- ‚è±Ô∏è Subsequent queries: 2-3 seconds
- ‚è±Ô∏è Vector retrieval alone: <50ms
- ‚è±Ô∏è Reload: 5-10 seconds

---

### Test 9.2: Memory Usage
**Objective:** Verify no memory leaks

**Steps:**
1. Open browser DevTools (F12)
2. Go to Memory tab
3. Ask 10+ questions in RAG chat
4. Observe memory usage

**Expected Result:**
- ‚úÖ Memory usage stable
- ‚úÖ No continuous growth
- ‚úÖ Garbage collection working
- ‚úÖ No memory leaks detected

---

## 10. Integration Tests

### Test 10.1: Both Chat Modes Work
**Objective:** Verify both chat modes functional simultaneously

**Steps:**
1. Use Standard Chat - ask for chart (e.g., "show agent performance")
2. Switch to RAG Chat - ask analytical question
3. Switch back to Standard Chat - predefined button
4. Switch to RAG - another question

**Expected Result:**
- ‚úÖ Standard Chat generates charts
- ‚úÖ RAG Chat provides semantic search
- ‚úÖ Both modes work correctly
- ‚úÖ No interference between modes
- ‚úÖ Independent history per tab

---

### Test 10.2: Data Consistency
**Objective:** Verify both modes use same summaries

**Steps:**
1. Note an agent name and score from Standard Chat
2. Ask RAG Chat about that agent
3. Verify same score is mentioned

**Expected Result:**
- ‚úÖ Scores match between modes
- ‚úÖ Agent names consistent
- ‚úÖ No data discrepancies
- ‚úÖ Same source for both modes

---

## 11. Documentation Tests

### Test 11.1: Code Documentation
**Objective:** Verify code is well-documented

**Checks:**
- ‚úÖ All classes have docstrings
- ‚úÖ All methods have docstrings
- ‚úÖ Parameters documented
- ‚úÖ Return values documented
- ‚úÖ Examples provided

---

### Test 11.2: User Documentation
**Objective:** Verify user guides are complete

**Checks:**
- ‚úÖ RAG_QUICK_START.md is clear
- ‚úÖ RAG_IMPLEMENTATION.md is detailed
- ‚úÖ Examples provided
- ‚úÖ Troubleshooting included
- ‚úÖ Screenshots/diagrams helpful

---

## Final Validation Checklist

- [ ] All 11 test categories passed
- [ ] No critical errors in logs
- [ ] No warning messages for users
- [ ] Both chat modes fully functional
- [ ] Performance acceptable (2-3 sec queries)
- [ ] UI clean and intuitive
- [ ] Documentation complete
- [ ] Code quality high
- [ ] Tests reproducible
- [ ] Ready for production

---

## Testing Commands (for developers)

### Check vector store files
```bash
ls -lah /Users/dineshwar.elanchezhian/Documents/Dev/Python/IK/Projects/capstone/cc-ai-summarizer/output_data/vector_store/
```

### Check logs for errors
```bash
tail -f /Users/dineshwar.elanchezhian/Documents/Dev/Python/IK/Projects/capstone/cc-ai-summarizer/logs/log_*.txt
```

### Test vector store programmatically
```python
from src.vector_store import VectorStoreManager

manager = VectorStoreManager()
manager.create_vector_store(api_key="your-key")
results = manager.similarity_search("which agents have high scores?", k=5)
print(results)
```

### Test RAG chatbot programmatically
```python
from src.rag_chat import RAGChatbot

chatbot = RAGChatbot(api_key="your-key")
chatbot.initialize()
response = chatbot.get_rag_response("Which agents have the highest scores?")
print(response)
```

---

## Test Report Template

Use this to document your test run:

```
RAG Implementation Test Report
Date: _______________
Tester: _______________
Environment: Python 3.14, Streamlit 1.50.0

Category 1 - Setup & Initialization: PASS / FAIL
Category 2 - Vector Store: PASS / FAIL
Category 3 - Similarity Search: PASS / FAIL
Category 4 - Chat History: PASS / FAIL
Category 5 - Context & LLM: PASS / FAIL
Category 6 - Predefined Questions: PASS / FAIL
Category 7 - UI/UX: PASS / FAIL
Category 8 - Error Handling: PASS / FAIL
Category 9 - Performance: PASS / FAIL
Category 10 - Integration: PASS / FAIL
Category 11 - Documentation: PASS / FAIL

Overall Result: PASS / FAIL

Issues Found:
1. [Description]
2. [Description]

Notes:
[Any additional observations]
```

---

**Happy testing! üß™**

